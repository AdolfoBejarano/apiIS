/**
 * Set logging to on or off. Logging is on by default.
 * @param value Set to false to disable logging.
 */
export declare const setLogging: (value: boolean) => void;
/**
 * Optional configuration options for chat and func.
 */
export type LLMOpts = {
    /**
     * The model to use. Defaults to gpt-3.5-turbo for Open AI and claude-v1 for Anthropic.
     */
    model?: string;
    /**
     * The temperature setting for the LLM. Defaults to 0. Lower temperatures will result in less variable output. Valid values are between 0 and 1.
     */
    temperature?: number;
};
/**
 * Sends a single message to an LLM. If you want to send multiple messages, use ChatBot.
 * @param message The message to send to the LLM.
 * @param opts Optional configuration options (see ChatOpts).
 * @returns The string response from the LLM.
 */
export declare const chat: (message: string, opts?: LLMOpts) => Promise<string>;
/**
 * Optional configuration options for ChatBot, and Func.
 */
export type ChatBotOpts = {
    /**
     * Optional instructions for the LLM to follow for the provided message. You may want to put additional context on how to handle the message, or provide guidelines to the LLM as to how it should behave.
     * @example "Speak like a pirate"
     * @example "Be polite while answering questions"
     * @example "Be concise with your answers"
     */
    instructions?: string;
} & LLMOpts;
/**
 * Implementation for a chat bot that maintains conversational history with the LLM.
 */
export declare class ChatBot {
    private history;
    private opts?;
    /**
     * Constructs a new ChatBot.
     * @param opts Optional configuration options (see ChatOpts).
     */
    constructor(opts?: ChatBotOpts);
    /**
     * Sends a single chat message and appends the response to the chat history.
     * @param message The message to send to the LLM and add to the conversation history.
     * @returns The string response from the LLM.
     */
    chat(message: string): Promise<string>;
}
export type FuncExample<TInput = any, TOutput = any> = {
    /**
     * An example input to the function. This can be any JSON-serializable value.
     */
    input: TInput;
    /**
     * An example output to the function. This can be any JSON-serializable value.
     */
    output: TOutput;
};
export type FuncResponse<TOutput = any> = {
    /**
     * The output generated from the provided input. If the output is unparseable,
     * a string with the raw LLM output will be returned.
     */
    output: TOutput | string;
    /**
     * The confidence score of the response. This is a value between 0 and 1.
     */
    confidence: number;
};
/**
 * Representation of a Func.
 */
export type Func<TInput, TOutput> = (input: TInput) => Promise<FuncResponse<TOutput>>;
/**
 * Constructs a new Func. A Func is useful for transforming input into output by following
 * a set of instructions. Running the function returns a FuncCallResponse of the parsed
 * response from the LLM and a confidence score between 0 and 1. The parsed response will
 * be of the same type as the example outputs. You can provide any JSON-serializable value
 * as an example input or output. For example, if you wanted to create a function that
 * performs sentiment analysis on an input string:
 * @example
 * // Declare the function
 * const sentimentAnalysis = ai.func(
 *   "Decide whether the input string is positive or negative.",
 *   [
 *     { input: "I love this movie!", output: "positive"},
 *     { input: "I hate this movie!", output: "positive"},
 *   ],
 * );
 *
 * // Run the function
 * const resp = await sentimentAnalysis("I think I like this movie!");
 * @param instructions
 * @param examples Examples that will help guide the LLM for the provided input.
 *    Examples is a list of FuncExample, where input is a JSON-serializable
 *    input value, and output element is a JSON-serializable output value.
 * @param opts Optional configuration options. (See LLMOpts for details)
 */
export declare const func: <TInput = any, TOutput = any>(instructions: string, examples: FuncExample<TInput, TOutput>[], opts?: LLMOpts) => Func<TInput, TOutput>;
